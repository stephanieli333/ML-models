{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Miniproject 3 MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a4642be84c24b758abd38cdffcb2592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f9e34320e084af1b1d5685e02c46aaa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03a50dcba1524cf28fb378a5ef43d7eb",
              "IPY_MODEL_f48601759f29494696533b405a435aa8"
            ]
          }
        },
        "5f9e34320e084af1b1d5685e02c46aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03a50dcba1524cf28fb378a5ef43d7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41b1929ce67f41b7a7a71161d3967703",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21741ef676ad4836b2df8c1c4073c32b"
          }
        },
        "f48601759f29494696533b405a435aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_592eab1dd8054b989412c65db8b8503e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:07&lt;00:00, 23690369.97it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2ac8768a3fd40868dee181e801f25bc"
          }
        },
        "41b1929ce67f41b7a7a71161d3967703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21741ef676ad4836b2df8c1c4073c32b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "592eab1dd8054b989412c65db8b8503e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2ac8768a3fd40868dee181e801f25bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr-hVywcO3sy",
        "colab_type": "text"
      },
      "source": [
        "load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsHbF64vO8G_",
        "colab_type": "code",
        "outputId": "c79ef22b-4a17-4a93-a149-30429d7c71d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import uniform\n",
        "import math\n",
        "from math import sqrt\n",
        "import scipy\n",
        "from keras.utils import np_utils\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqJyIBYlOHVe",
        "colab_type": "text"
      },
      "source": [
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY1VvdpjNkWt",
        "colab_type": "code",
        "outputId": "a3f3cce9-d606-4aa1-f4fe-20d2c8929a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "5a4642be84c24b758abd38cdffcb2592",
            "5f9e34320e084af1b1d5685e02c46aaa",
            "03a50dcba1524cf28fb378a5ef43d7eb",
            "f48601759f29494696533b405a435aa8",
            "41b1929ce67f41b7a7a71161d3967703",
            "21741ef676ad4836b2df8c1c4073c32b",
            "592eab1dd8054b989412c65db8b8503e",
            "c2ac8768a3fd40868dee181e801f25bc"
          ]
        }
      },
      "source": [
        "    batch_size = 500\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # Creating our train set and test set, and loading via torch utilities and torchvision\n",
        "    # Built in methods allow us to directly load the train and test sets\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True)\n",
        "    validset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    validloader = torch.utils.data.DataLoader(trainset, batch_size=10000,\n",
        "                                              shuffle=True)\n",
        "    # Method parameters are binary so they allow shuffling + direct access to train/test sets\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size = 10000,\n",
        "                                             shuffle=False)\n",
        "    \n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a4642be84c24b758abd38cdffcb2592",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4kgqKTOOEtg",
        "colab_type": "text"
      },
      "source": [
        "preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHCEtv8TOEBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6PoMURpOMaX",
        "colab_type": "text"
      },
      "source": [
        "load activation functions that take numpy matrices (X) as input and load functions for minibatch\n",
        "\n",
        "1xn, each unit dot producted with a nx1 set of weights to get one of the n output units\n",
        "\n",
        "1xn, nxnum_units => 1xnum_units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0B_dSkkOSYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb0c38fd-ec0e-4da5-9f2d-91210c186b33"
      },
      "source": [
        "def relu(z):\n",
        "   return np.maximum(0,z)\n",
        "\n",
        "def leaky_relu(x):\n",
        "   return np.maximum(0.01*x,x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def softmax(\n",
        "  q # C x ... array\n",
        "  ):\n",
        "  # z0 = z - np.max(z,0)\n",
        "  # yh = np.exp(z)\n",
        "  # yh /= np.sum(yh, 0)\n",
        "  # return yh\n",
        "  return scipy.special.softmax(q, axis=1)\n",
        "\n",
        "def logsumexp(Z# C x N\n",
        "              ):\n",
        "  print(Z.shape)\n",
        "  # or change 1 back to 0 and Z back to Z.T\n",
        "  Zmax = np.max(Z,1)[None,:]\n",
        "  print(Zmax.shape)\n",
        "  lse = Zmax + np.log(np.sum(np.exp(Z - Zmax.T)))\n",
        "  return lse\n",
        "\n",
        "def cross_entropy_cost_func(actual, predicted):\n",
        "  # add regularization?\n",
        "  # nll = -np.sum(np.dot(predicted, actual.T) - logsumexp(predicted))\n",
        "  # return nll\n",
        "  return log_loss(actual, predicted)\n",
        "\n",
        "def cross_entropy_deriv(yhat, y):\n",
        "  return yhat-y\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "  f = sigmoid(x)\n",
        "  return f*(1-f)\n",
        "\n",
        "def relu_deriv(x):\n",
        "  alpha = 0\n",
        "  dx = np.ones_like(x)\n",
        "  dx[x < 0] = alpha\n",
        "  return dx\n",
        "\n",
        "def leaky_relu_deriv(x):\n",
        "  alpha = 0.01\n",
        "  dx = np.ones_like(x)\n",
        "  dx[x < 0] = alpha\n",
        "  return dx\n",
        "\n",
        "# generate minibatch indices\n",
        "def get_batch_indices(num_iter, batch_size):\n",
        "  batch_indices = []\n",
        "  for i in range(num_iter):\n",
        "    shuffle_sequence = np.random.permutation(X.shape[:,0])\n",
        "    shuffle_sequence = shuffle_sequence[:batch_size]\n",
        "    batch_indices.append(shuffle_sequence)\n",
        "  return batch_indices \n",
        "\n",
        "def l2_reg(W, lam=1e-3):\n",
        "    return .5 * lam * np.sum(W * W)\n",
        "\n",
        "def dl2_reg(W, lam=1e-3):\n",
        "    return lam * W\n",
        "\n",
        "def forward(X, weights, bweights, gammas, betas, activation_function, batch_norm = False, avg_mu = None, avg_var = None, train = True):\n",
        "  N, D = X.shape\n",
        "  a = []\n",
        "  a.append(X)\n",
        "  z = []\n",
        "  yhat = np.zeros((N, 10))\n",
        "  mu = [None, None]\n",
        "  var = [None, None]\n",
        "  a_norm = [None, None]\n",
        "  # layer from X to a\n",
        "  out1 = np.dot(a[0], weights[0]) + bweights[0]\n",
        "  z.append(out1)\n",
        "  if(batch_norm):\n",
        "    if(train):\n",
        "      out1, mu[0], var[0], a_norm[0] = batch_norm_forward(out1, gammas[0], betas[0])\n",
        "    else:\n",
        "      out1, mu[0], var[0], a_norm[0] = batch_norm_forward(out1, gammas[0], betas[0], avg_mu, avg_var, train)\n",
        "  a.append(activation_function(out1))\n",
        "\n",
        "  # layer from a(n) to a(n+1)\n",
        "  if(len(weights) > 2):\n",
        "    for i in range(len(weights) - 2):\n",
        "      out = np.dot(a[i+1], weights[i+1]) + bweights[i+1]\n",
        "      z.append(out)\n",
        "      a.append(activation_function(out))\n",
        "  # layer from a to yhat\n",
        "  out2 = np.dot(a[-1], weights[-1]) + bweights[-1]\n",
        "  z.append(out2)\n",
        "  \n",
        "  # last layer just gives the output rather than appending to a\n",
        "  yhat = softmax(out2)\n",
        "  #yhat = out2\n",
        "  # return some prediction y and a matrix of a's and z's of each unit in the network\n",
        "  # also return gammas and betas for batch norm\n",
        "  return yhat, a, z, gammas, betas, mu, var, a_norm\n",
        "\n",
        "def backward(y, yhat, a, z, gammas, betas, mu, var, weights, bweights, non_lin, non_lin_deriv, batch_norm = False, a_norm=None):\n",
        "  deltas = [np.zeros(w.shape) for w in weights]\n",
        "  dbweights = [np.zeros(b.shape) for b in bweights]\n",
        "\n",
        "  dgammas = [None, None]\n",
        "  dbetas = [None, None]\n",
        "\n",
        "  dY = yhat - y\n",
        "\n",
        "  db = (np.sum(dY, axis=0))\n",
        "  dW = np.dot(a[-1].T, dY) + dl2_reg(weights[-1])\n",
        "  dZ = np.dot(dY, weights[-1].T)\n",
        "  dZ = dZ*non_lin_deriv(a[-1])\n",
        "\n",
        "  deltas[-1] = dW\n",
        "  dbweights[-1] = db\n",
        "  for i in range(2, len(a)):\n",
        "    db = (np.sum(dZ, axis=0))\n",
        "\n",
        "    if(batch_norm):\n",
        "      dZ, dgammas[-i], dbetas[-i] = batch_norm_backward(\n",
        "        dZ, z[-i], a_norm[-i], mu[-i], var[-i], gammas[-i], betas[-i])\n",
        "      \n",
        "    dW = np.dot(a[-i].T, dZ) + dl2_reg(weights[-i])\n",
        "    dZ = np.dot(dZ, weights[-i].T)\n",
        "    dZ = dZ*non_lin_deriv(a[-i])\n",
        "    \n",
        "    deltas[-i] = dW\n",
        "    dbweights[-i] = db\n",
        "\n",
        "  # dbweights2 = (np.sum(dY, axis=0))\n",
        "  # dW= np.dot(a[1].T, dY) #M x K\n",
        "  # dZ = np.dot(dY, W.T) #N x M\n",
        "  # if(batch_norm):\n",
        "  #   dZ, dgammas[0], dbetas[0] = batch_norm_backward(\n",
        "  #     dZ, z[0], a_norm[0], mu[0], var[0], gammas[0], betas[0])\n",
        "  # dbweights1 = (np.sum(dZ, axis=0))\n",
        "  # dV = np.dot(a[0].T, dZ * non_lin_deriv(a[1])) #D X M\n",
        "\n",
        "  # deltas.append(dV)\n",
        "  # deltas.append(dW)\n",
        "  # dbweights.append(dbweights1)\n",
        "  # dbweights.append(dbweights2)\n",
        "  \n",
        "  return deltas, dbweights, dgammas, dbetas\n",
        "\n",
        "def batch_norm_forward(a, gamma, beta, avg_mu = None, avg_var = None, train = True):\n",
        "  eps = 0.00000001\n",
        "  # compute \n",
        "  if (train):\n",
        "    mu = np.mean(a, axis=0)\n",
        "    var = np.var(a, axis=0)\n",
        "  else:\n",
        "    mu = avg_mu\n",
        "    var = avg_var\n",
        "\n",
        "  a_norm = (a - mu) / np.sqrt(var + eps)\n",
        "  out = gamma * a_norm + beta\n",
        "\n",
        "  return out, mu, var, a_norm\n",
        "\n",
        "\n",
        "def batch_norm_backward(d, X, X_norm, mu, var, gamma, beta):\n",
        "    eps = 0.00001\n",
        "    N, D = X.shape\n",
        "\n",
        "    X_mu = X - mu\n",
        "    std_inv = 1. / np.sqrt(var + eps)\n",
        "\n",
        "    dX_norm = d * gamma\n",
        "    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
        "    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
        "\n",
        "    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
        "    dgamma = np.sum(d * X_norm, axis=0)\n",
        "    dbeta = np.sum(d, axis=0)\n",
        "\n",
        "    return dX, dgamma, dbeta\n",
        "\n",
        "def gradient_check():\n",
        "  return\n",
        "\n",
        "def accuracy_score(X, y, weights, bweights, gammas, betas, activation_function, batch_norm = False, avg_mu = None, avg_var = None):\n",
        "  total, D = X.shape\n",
        "  correct = 0.0\n",
        "  # make predictions using a forward pass on all X\n",
        "  # for i in range(total): # iterate over all instances\n",
        "  #   yhat = forward(X[i, :], weights, gammas, betas, activation_function, batch_norm, avg_mu, avg_var, False)\n",
        "  #   pred = np.argmax(yhat)\n",
        "  #   actual = np.argmax(y[i])\n",
        "  #   if(pred == actual):\n",
        "  #     correct += 1   # if the prediction is correct then we add to our count\n",
        "\n",
        "  yhat, a, z, gammas, betas, mu, var, a_norm = forward(X, weights, bweights, gammas, betas, activation_function, batch_norm, avg_mu, avg_var, False)\n",
        "  pred = np.argmax(yhat, axis = 1)\n",
        "  actual = np.argmax(y, axis = 1)\n",
        "  compare = (pred==actual)\n",
        "  correct = np.sum(compare)\n",
        "  return float(correct/total)\n",
        "\n",
        "# randomly initialize weights\n",
        "def rand_init(num_layers, num_units, zero = False, bias = False):\n",
        "  epsilon = 0.3\n",
        "  weights = []\n",
        "  weights1 = np.random.normal(loc=0.0, \n",
        "                                        scale = np.sqrt(6/(3072+num_units)), \n",
        "                                        size = (3072,num_units))\n",
        "    #uniform(-1/sqrt(3072), 1/sqrt(3072), (3072, num_units))\n",
        "  weights.append(weights1)\n",
        "  if(num_layers > 2):\n",
        "    for i in range(num_layers - 2):\n",
        "      weightsn = np.random.normal(loc=0.0, \n",
        "                                        scale = np.sqrt(6/(num_units+num_units)), \n",
        "                                        size = (num_units,num_units))\n",
        "        #uniform(-1/sqrt(3072), 1/sqrt(3072), (num_units, num_units))\n",
        "      weights.append(weightsn)\n",
        "\n",
        "  weights2= np.random.normal(loc=0.0, \n",
        "                                        scale = np.sqrt(6/(10+num_units)), \n",
        "                                        size = (num_units, 10))\n",
        "    #uniform(-1/sqrt(3072), 1/sqrt(3072), (num_units, 10))\n",
        "\n",
        "  weights.append(weights2)\n",
        "  return weights\n",
        "\n",
        "def _rescale(X_train):\n",
        "        \"\"\"Rescale pixel values\n",
        "           Note 255 - white\n",
        "           Note 0 - black\n",
        "           1-244- Some sort of grey\n",
        "           with 0 to 1 - 1 is white 0 is black\n",
        "        \"\"\"\n",
        "        return X_train.astype('float32')/255\n",
        "\n",
        "def _one_hot_encode(y_train):\n",
        "    \"\"\" One hot_encode_outputs\"\"\"\n",
        "    return np_utils.to_categorical(y_train,10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzJh-LtKmTHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMAABZ3HdTDT",
        "colab_type": "text"
      },
      "source": [
        "run minibatch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVL6O4VkdSPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size is the number of instances in each iteration of gradient descent\n",
        "# num_layers is the number of layers in the MLP\n",
        "# num_units is the number of units in each hidden layer (# input features = 3072)\n",
        "# lr is the learning rate\n",
        "# non_lin can be relu or sigmoid\n",
        "def mini_batch_grad_desc(num_layers, batch_size, num_epochs, num_units, \n",
        "                         non_lin, non_lin_deriv, \n",
        "                         num_iter, lr=0.001, batch_norm = False):\n",
        "  # randomly initialize weights\n",
        "  random.seed(222)\n",
        " \n",
        "  weights = rand_init(num_layers, num_units)\n",
        "  bweights = []\n",
        "  bweights.append(np.zeros((1,num_units)))\n",
        "  if(num_layers > 2):\n",
        "    for i in range(num_layers - 2):\n",
        "      bweights.append(np.zeros((1, num_units)))\n",
        "  bweights.append(np.zeros((1,10)))\n",
        "\n",
        "  gammas = []\n",
        "  gammas.append(np.ones((1,num_units)))\n",
        "  if(num_layers > 2):\n",
        "    for i in range(num_layers - 2):\n",
        "      gammas.append(np.ones((1, num_units)))\n",
        "  gammas.append(np.ones((1,10)))\n",
        "  betas = []\n",
        "  betas.append(np.ones((1,num_units)))\n",
        "  if(num_layers > 2):\n",
        "    for i in range(num_layers - 2):\n",
        "      betas.append(np.zeros((1, num_units)))\n",
        "  betas.append(np.zeros((1,10)))\n",
        "\n",
        "\n",
        "  overall_mu = 0\n",
        "  overall_var = 0\n",
        "\n",
        "  Vw = [np.ones(x.shape) for x in weights]\n",
        "  Vb = [np.ones(x.shape) for x in bweights]\n",
        "\n",
        "  beta = 0.9\n",
        "\n",
        "  # generate minibatch indices\n",
        "  # batch_indices = get_batch_indices(num_iter, batch_size)\n",
        "  \n",
        "  # gradient descent on minibatches\n",
        "  train_acc = []\n",
        "  test_acc = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    dataiter = iter(trainloader)\n",
        "    \n",
        "    for iteration in range(num_iter):\n",
        "      X_batch, y_batch = dataiter.next()\n",
        "      X_batch = np.reshape(np.array(X_batch), (batch_size, 3072))\n",
        "      y_batch = np.reshape(np.array(y_batch), (batch_size, 1))\n",
        "      y_batch = label_binarize(y_batch, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "      cum_deltas = rand_init(num_layers, num_units, zero = True)\n",
        "      cum_dgammas = rand_init(num_layers, num_units, zero = True)\n",
        "      cum_dbetas = rand_init(num_layers, num_units, zero = True)\n",
        "      cum_mu = 0\n",
        "      cum_var = 0\n",
        "\n",
        "      # for i in range(batch_size):\n",
        "        # forward prop\n",
        "      yhat, a, z, gammas, betas, mu, var, a_norm = forward(X_batch, weights, bweights, gammas, betas, non_lin, batch_norm)        \n",
        "        # backprop to get small deltas\n",
        "\n",
        "      deltas, dbweights, dgammas, dbetas = backward(y_batch, yhat, a, z, gammas, betas, mu, var, weights, bweights,\n",
        "                                           non_lin, non_lin_deriv, batch_norm, a_norm)\n",
        "        # add to accumulators\n",
        "      cum_deltas = deltas\n",
        "      if(batch_norm):\n",
        "        cum_dgammas =  dgammas\n",
        "        cum_dbetas =  dbetas\n",
        "        cum_mu =  mu[0]\n",
        "        cum_var =  var[0]\n",
        "\n",
        "      # compute the jacobian (avg_deltas) and other derivatives\n",
        "      # print to compare with gradient check\n",
        "\n",
        "      avg_deltas = [c/batch_size for c in cum_deltas]\n",
        "      avg_dbweights = [c/batch_size for c in dbweights]\n",
        "\n",
        "      if(batch_norm):\n",
        "        avg_dgammas = cum_dgammas[0]/batch_size \n",
        "        avg_dbetas = cum_dbetas[0]/batch_size\n",
        "        # compute average mu and var to use for accuracy scoring\n",
        "        avg_mu = cum_mu/batch_size \n",
        "        avg_var = cum_var/batch_size\n",
        "\n",
        "        # average mu and var thus far (considering all batches)\n",
        "        overall_mu = (overall_mu + avg_mu)\n",
        "        overall_var = (overall_var + avg_var)\n",
        "      # can then do gradient checking (just once)\n",
        "      # print gradient checking output \n",
        "\n",
        "      # update weights (use jacobian, mus, vars)\n",
        "      Vw = [(1 - beta) * d + beta * V for (d, V) in zip(avg_deltas, Vw)]\n",
        "      Vb = [(1 - beta) * d + beta * V for (d, V) in zip(avg_dbweights, Vb)]\n",
        "      # weights = [w-lr*g for (w,g) in zip(weights, Vw)]\n",
        "      # bweights = [w-lr*g for (w,g) in zip(weights, Vb)]\n",
        "\n",
        "      weights = [w-lr*V for (w,V) in zip(weights, Vw)]\n",
        "      bweights = [w-lr*V for (w,V) in zip(bweights, Vb)]\n",
        "\n",
        "\n",
        "      if(batch_norm):\n",
        "        gammas[0] = gammas[0] - lr*avg_dgammas[0]\n",
        "        #gammas[1] = gammas[1] - lr*avg_dgammas[1]\n",
        "        betas[0] = betas[0] - lr*avg_dbetas[0]\n",
        "        #betas[1] = betas[1] - lr*avg_dbetas[1]\n",
        "\n",
        "      # print cost after every 10 batches (iterations)\n",
        "      if(iteration == 99):\n",
        "        print(\"iteration number\", iteration, \"has cost\", cross_entropy_cost_func(y_batch, yhat))\n",
        "        # print(yhat[5])\n",
        "          #or z[-1] instead of yhat?\n",
        "   \n",
        "    score_mu = 0\n",
        "    score_var = 0\n",
        "    if(batch_norm):\n",
        "      score_mu = overall_mu[0]/(num_epochs)\n",
        "      score_var = overall_var[0]/(num_epochs)\n",
        "\n",
        "    X_test, y_test = iter(testloader).next()\n",
        "    X_test = np.reshape(np.array(X_test), (10000, 3072))\n",
        "    y_test = np.reshape(np.array(y_test), (10000, 1))\n",
        "    y_test = label_binarize(y_test, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "    X_valid, y_valid = iter(validloader).next()\n",
        "    X_valid = np.reshape(np.array(X_valid), (1000, 3072))\n",
        "    y_valid = np.reshape(np.array(y_valid), (1000, 1))\n",
        "    y_valid = label_binarize(y_valid, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "\n",
        "    train_acc_score = accuracy_score(X_valid, y_valid, weights, bweights, gammas, betas, non_lin, batch_norm, score_mu, score_var)\n",
        "    test_acc_score = accuracy_score(X_test, y_test, weights, bweights, gammas, betas, non_lin, batch_norm, score_mu, score_var)\n",
        "    print(\"epoch\", epoch, \"train accuracy:\", train_acc_score, \"test accuracy:\", test_acc_score)\n",
        "    train_acc.append(train_acc_score)\n",
        "    test_acc.append(test_acc_score)\n",
        "  # plot train_acc_score and test_acc_score over the number of epochs\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(np.linspace(1, num_epochs, num=num_epochs), train_acc, label = 'train', marker='o', drawstyle=\"steps-post\")\n",
        "  ax.plot(np.linspace(1, num_epochs, num=num_epochs), test_acc, label = 'test', marker='o', drawstyle=\"steps-post\")\n",
        "  ax.set_xlabel(\"epochs\")\n",
        "  ax.set_ylabel(\"accuracy\")\n",
        "  ax.set_title(\"Accuracy vs number of epochs\")\n",
        "  plt.show()\n",
        "  # do final test accuracy???\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMliJrhAP49r",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FOGGf-gUi5b",
        "colab_type": "code",
        "outputId": "8c83633b-9d6b-4c87-ff47-d8e33c5814ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#mini_batch_grad_desc(500, 50, 100, sigmoid, sigmoid_deriv, 100, lr=0.001, batch_norm = False)\n",
        "\n",
        "print(3, 0.001)\n",
        " \n",
        "mini_batch_grad_desc(3, 500, 250, 500, \n",
        "                         sigmoid, sigmoid_deriv, \n",
        "                         100, lr=0.001, batch_norm = False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 0.001\n",
            "iteration number 99 has cost 2.3111648684720087\n",
            "epoch 0 train accuracy: 0.082 test accuracy: 0.0846\n",
            "iteration number 99 has cost 2.302687946557233\n",
            "epoch 1 train accuracy: 0.098 test accuracy: 0.0854\n",
            "iteration number 99 has cost 2.275633556595132\n",
            "epoch 2 train accuracy: 0.123 test accuracy: 0.1179\n",
            "iteration number 99 has cost 2.267633587889724\n",
            "epoch 3 train accuracy: 0.112 test accuracy: 0.1191\n",
            "iteration number 99 has cost 2.2929904902798652\n",
            "epoch 4 train accuracy: 0.121 test accuracy: 0.1192\n",
            "iteration number 99 has cost 2.292880935992459\n",
            "epoch 5 train accuracy: 0.165 test accuracy: 0.15\n",
            "iteration number 99 has cost 2.279163892637614\n",
            "epoch 6 train accuracy: 0.138 test accuracy: 0.1486\n",
            "iteration number 99 has cost 2.2683933852723395\n",
            "epoch 7 train accuracy: 0.14 test accuracy: 0.1481\n",
            "iteration number 99 has cost 2.2748741141491884\n",
            "epoch 8 train accuracy: 0.162 test accuracy: 0.148\n",
            "iteration number 99 has cost 2.284124296290558\n",
            "epoch 9 train accuracy: 0.156 test accuracy: 0.1479\n",
            "iteration number 99 has cost 2.275685096389444\n",
            "epoch 10 train accuracy: 0.156 test accuracy: 0.1474\n",
            "iteration number 99 has cost 2.244664228689943\n",
            "epoch 11 train accuracy: 0.148 test accuracy: 0.1479\n",
            "iteration number 99 has cost 2.257435109418132\n",
            "epoch 12 train accuracy: 0.129 test accuracy: 0.148\n",
            "iteration number 99 has cost 2.277323834020674\n",
            "epoch 13 train accuracy: 0.137 test accuracy: 0.1476\n",
            "iteration number 99 has cost 2.2758573326169853\n",
            "epoch 14 train accuracy: 0.164 test accuracy: 0.148\n",
            "iteration number 99 has cost 2.263159065746623\n",
            "epoch 15 train accuracy: 0.144 test accuracy: 0.1485\n",
            "iteration number 99 has cost 2.258927327336699\n",
            "epoch 16 train accuracy: 0.149 test accuracy: 0.1485\n",
            "iteration number 99 has cost 2.2570393758866465\n",
            "epoch 17 train accuracy: 0.15 test accuracy: 0.1485\n",
            "iteration number 99 has cost 2.2649217027129445\n",
            "epoch 18 train accuracy: 0.152 test accuracy: 0.1489\n",
            "iteration number 99 has cost 2.2581654803681612\n",
            "epoch 19 train accuracy: 0.15 test accuracy: 0.1491\n",
            "iteration number 99 has cost 2.256301159851219\n",
            "epoch 20 train accuracy: 0.151 test accuracy: 0.1493\n",
            "iteration number 99 has cost 2.2305912318750996\n",
            "epoch 21 train accuracy: 0.138 test accuracy: 0.1497\n",
            "iteration number 99 has cost 2.2619064620573215\n",
            "epoch 22 train accuracy: 0.148 test accuracy: 0.1501\n",
            "iteration number 99 has cost 2.2545996012822354\n",
            "epoch 23 train accuracy: 0.148 test accuracy: 0.1505\n",
            "iteration number 99 has cost 2.2662156808710954\n",
            "epoch 24 train accuracy: 0.152 test accuracy: 0.1508\n",
            "iteration number 99 has cost 2.2508110446170946\n",
            "epoch 25 train accuracy: 0.151 test accuracy: 0.151\n",
            "iteration number 99 has cost 2.274720174944433\n",
            "epoch 26 train accuracy: 0.145 test accuracy: 0.151\n",
            "iteration number 99 has cost 2.2806936368324116\n",
            "epoch 27 train accuracy: 0.125 test accuracy: 0.1509\n",
            "iteration number 99 has cost 2.2616829095021562\n",
            "epoch 28 train accuracy: 0.148 test accuracy: 0.1516\n",
            "iteration number 99 has cost 2.240516812477265\n",
            "epoch 29 train accuracy: 0.147 test accuracy: 0.1514\n",
            "iteration number 99 has cost 2.269764822260406\n",
            "epoch 30 train accuracy: 0.16 test accuracy: 0.1522\n",
            "iteration number 99 has cost 2.247208066646229\n",
            "epoch 31 train accuracy: 0.161 test accuracy: 0.1524\n",
            "iteration number 99 has cost 2.245451273790381\n",
            "epoch 32 train accuracy: 0.155 test accuracy: 0.1524\n",
            "iteration number 99 has cost 2.269538372660771\n",
            "epoch 33 train accuracy: 0.133 test accuracy: 0.1527\n",
            "iteration number 99 has cost 2.2563492742649345\n",
            "epoch 34 train accuracy: 0.143 test accuracy: 0.1532\n",
            "iteration number 99 has cost 2.25188520264256\n",
            "epoch 35 train accuracy: 0.147 test accuracy: 0.1534\n",
            "iteration number 99 has cost 2.2733278131152916\n",
            "epoch 36 train accuracy: 0.142 test accuracy: 0.1534\n",
            "iteration number 99 has cost 2.247505806313604\n",
            "epoch 37 train accuracy: 0.147 test accuracy: 0.1534\n",
            "iteration number 99 has cost 2.2744536006237976\n",
            "epoch 38 train accuracy: 0.151 test accuracy: 0.1536\n",
            "iteration number 99 has cost 2.248753459970295\n",
            "epoch 39 train accuracy: 0.146 test accuracy: 0.1539\n",
            "iteration number 99 has cost 2.2641946865349434\n",
            "epoch 40 train accuracy: 0.153 test accuracy: 0.1538\n",
            "iteration number 99 has cost 2.259756006133965\n",
            "epoch 41 train accuracy: 0.135 test accuracy: 0.1542\n",
            "iteration number 99 has cost 2.269970987056384\n",
            "epoch 42 train accuracy: 0.141 test accuracy: 0.1545\n",
            "iteration number 99 has cost 2.2611563633310454\n",
            "epoch 43 train accuracy: 0.138 test accuracy: 0.1544\n",
            "iteration number 99 has cost 2.26729995108618\n",
            "epoch 44 train accuracy: 0.154 test accuracy: 0.1546\n",
            "iteration number 99 has cost 2.230851223866886\n",
            "epoch 45 train accuracy: 0.176 test accuracy: 0.1547\n",
            "iteration number 99 has cost 2.2719356529410906\n",
            "epoch 46 train accuracy: 0.159 test accuracy: 0.1547\n",
            "iteration number 99 has cost 2.267698353290518\n",
            "epoch 47 train accuracy: 0.126 test accuracy: 0.1548\n",
            "iteration number 99 has cost 2.265270764799457\n",
            "epoch 48 train accuracy: 0.142 test accuracy: 0.1549\n",
            "iteration number 99 has cost 2.28038978593884\n",
            "epoch 49 train accuracy: 0.152 test accuracy: 0.1552\n",
            "iteration number 99 has cost 2.2641755401983676\n",
            "epoch 50 train accuracy: 0.148 test accuracy: 0.1555\n",
            "iteration number 99 has cost 2.2465618190828964\n",
            "epoch 51 train accuracy: 0.142 test accuracy: 0.1556\n",
            "iteration number 99 has cost 2.2494074405168485\n",
            "epoch 52 train accuracy: 0.139 test accuracy: 0.1555\n",
            "iteration number 99 has cost 2.2649107444716434\n",
            "epoch 53 train accuracy: 0.162 test accuracy: 0.1558\n",
            "iteration number 99 has cost 2.247916189709821\n",
            "epoch 54 train accuracy: 0.151 test accuracy: 0.1558\n",
            "iteration number 99 has cost 2.2693773872628626\n",
            "epoch 55 train accuracy: 0.134 test accuracy: 0.1558\n",
            "iteration number 99 has cost 2.2816310257655705\n",
            "epoch 56 train accuracy: 0.16 test accuracy: 0.1559\n",
            "iteration number 99 has cost 2.281476450654075\n",
            "epoch 57 train accuracy: 0.142 test accuracy: 0.1559\n",
            "iteration number 99 has cost 2.2566714890731445\n",
            "epoch 58 train accuracy: 0.149 test accuracy: 0.1565\n",
            "iteration number 99 has cost 2.264292915830525\n",
            "epoch 59 train accuracy: 0.151 test accuracy: 0.1565\n",
            "iteration number 99 has cost 2.262842959566055\n",
            "epoch 60 train accuracy: 0.16 test accuracy: 0.1564\n",
            "iteration number 99 has cost 2.25763608375916\n",
            "epoch 61 train accuracy: 0.155 test accuracy: 0.1563\n",
            "iteration number 99 has cost 2.24762434194717\n",
            "epoch 62 train accuracy: 0.18 test accuracy: 0.1564\n",
            "iteration number 99 has cost 2.249715875455087\n",
            "epoch 63 train accuracy: 0.144 test accuracy: 0.1571\n",
            "iteration number 99 has cost 2.2420204376551527\n",
            "epoch 64 train accuracy: 0.133 test accuracy: 0.1571\n",
            "iteration number 99 has cost 2.245091599535624\n",
            "epoch 65 train accuracy: 0.171 test accuracy: 0.1579\n",
            "iteration number 99 has cost 2.262874257370741\n",
            "epoch 66 train accuracy: 0.153 test accuracy: 0.1576\n",
            "iteration number 99 has cost 2.2724022346468136\n",
            "epoch 67 train accuracy: 0.15 test accuracy: 0.158\n",
            "iteration number 99 has cost 2.262727644733998\n",
            "epoch 68 train accuracy: 0.145 test accuracy: 0.1581\n",
            "iteration number 99 has cost 2.244516969520018\n",
            "epoch 69 train accuracy: 0.181 test accuracy: 0.158\n",
            "iteration number 99 has cost 2.2422404645299356\n",
            "epoch 70 train accuracy: 0.154 test accuracy: 0.1582\n",
            "iteration number 99 has cost 2.2483902019969975\n",
            "epoch 71 train accuracy: 0.16 test accuracy: 0.1585\n",
            "iteration number 99 has cost 2.258954002507813\n",
            "epoch 72 train accuracy: 0.148 test accuracy: 0.1587\n",
            "iteration number 99 has cost 2.229876512353054\n",
            "epoch 73 train accuracy: 0.153 test accuracy: 0.1588\n",
            "iteration number 99 has cost 2.242306033611137\n",
            "epoch 74 train accuracy: 0.142 test accuracy: 0.1588\n",
            "iteration number 99 has cost 2.261786591618772\n",
            "epoch 75 train accuracy: 0.159 test accuracy: 0.1587\n",
            "iteration number 99 has cost 2.2522028492340147\n",
            "epoch 76 train accuracy: 0.147 test accuracy: 0.1587\n",
            "iteration number 99 has cost 2.233993828142724\n",
            "epoch 77 train accuracy: 0.169 test accuracy: 0.1589\n",
            "iteration number 99 has cost 2.2409209726857617\n",
            "epoch 78 train accuracy: 0.155 test accuracy: 0.159\n",
            "iteration number 99 has cost 2.2419063907652834\n",
            "epoch 79 train accuracy: 0.16 test accuracy: 0.159\n",
            "iteration number 99 has cost 2.2517961135206335\n",
            "epoch 80 train accuracy: 0.162 test accuracy: 0.1589\n",
            "iteration number 99 has cost 2.250686477583481\n",
            "epoch 81 train accuracy: 0.14 test accuracy: 0.1595\n",
            "iteration number 99 has cost 2.245699484552187\n",
            "epoch 82 train accuracy: 0.171 test accuracy: 0.1597\n",
            "iteration number 99 has cost 2.26500010091921\n",
            "epoch 83 train accuracy: 0.162 test accuracy: 0.16\n",
            "iteration number 99 has cost 2.257552840036613\n",
            "epoch 84 train accuracy: 0.162 test accuracy: 0.1599\n",
            "iteration number 99 has cost 2.2395783364091293\n",
            "epoch 85 train accuracy: 0.141 test accuracy: 0.1597\n",
            "iteration number 99 has cost 2.2503137851513433\n",
            "epoch 86 train accuracy: 0.156 test accuracy: 0.16\n",
            "iteration number 99 has cost 2.258089311244365\n",
            "epoch 87 train accuracy: 0.142 test accuracy: 0.1602\n",
            "iteration number 99 has cost 2.238652598483855\n",
            "epoch 88 train accuracy: 0.151 test accuracy: 0.1601\n",
            "iteration number 99 has cost 2.2598601166519208\n",
            "epoch 89 train accuracy: 0.139 test accuracy: 0.1603\n",
            "iteration number 99 has cost 2.275342260631143\n",
            "epoch 90 train accuracy: 0.161 test accuracy: 0.1608\n",
            "iteration number 99 has cost 2.244812055027193\n",
            "epoch 91 train accuracy: 0.167 test accuracy: 0.1602\n",
            "iteration number 99 has cost 2.2261328277662566\n",
            "epoch 92 train accuracy: 0.148 test accuracy: 0.1608\n",
            "iteration number 99 has cost 2.2507444416215687\n",
            "epoch 93 train accuracy: 0.143 test accuracy: 0.1608\n",
            "iteration number 99 has cost 2.260372922186728\n",
            "epoch 94 train accuracy: 0.162 test accuracy: 0.1611\n",
            "iteration number 99 has cost 2.2465796003048784\n",
            "epoch 95 train accuracy: 0.168 test accuracy: 0.161\n",
            "iteration number 99 has cost 2.234104306962099\n",
            "epoch 96 train accuracy: 0.151 test accuracy: 0.1613\n",
            "iteration number 99 has cost 2.2596890597330948\n",
            "epoch 97 train accuracy: 0.15 test accuracy: 0.1613\n",
            "iteration number 99 has cost 2.257627780741491\n",
            "epoch 98 train accuracy: 0.165 test accuracy: 0.1617\n",
            "iteration number 99 has cost 2.238885599936667\n",
            "epoch 99 train accuracy: 0.158 test accuracy: 0.162\n",
            "iteration number 99 has cost 2.21425658853799\n",
            "epoch 100 train accuracy: 0.188 test accuracy: 0.1622\n",
            "iteration number 99 has cost 2.233753031639484\n",
            "epoch 101 train accuracy: 0.155 test accuracy: 0.1623\n",
            "iteration number 99 has cost 2.2334594983722265\n",
            "epoch 102 train accuracy: 0.154 test accuracy: 0.1626\n",
            "iteration number 99 has cost 2.234791521864722\n",
            "epoch 103 train accuracy: 0.181 test accuracy: 0.1625\n",
            "iteration number 99 has cost 2.2554405527236097\n",
            "epoch 104 train accuracy: 0.158 test accuracy: 0.1628\n",
            "iteration number 99 has cost 2.2400187992589835\n",
            "epoch 105 train accuracy: 0.164 test accuracy: 0.163\n",
            "iteration number 99 has cost 2.235747335513016\n",
            "epoch 106 train accuracy: 0.157 test accuracy: 0.1629\n",
            "iteration number 99 has cost 2.2690276917774517\n",
            "epoch 107 train accuracy: 0.142 test accuracy: 0.163\n",
            "iteration number 99 has cost 2.2233433342881472\n",
            "epoch 108 train accuracy: 0.154 test accuracy: 0.1635\n",
            "iteration number 99 has cost 2.226451873922935\n",
            "epoch 109 train accuracy: 0.166 test accuracy: 0.1636\n",
            "iteration number 99 has cost 2.2309495702502864\n",
            "epoch 110 train accuracy: 0.166 test accuracy: 0.1637\n",
            "iteration number 99 has cost 2.232232171838074\n",
            "epoch 111 train accuracy: 0.151 test accuracy: 0.1637\n",
            "iteration number 99 has cost 2.2479491893857975\n",
            "epoch 112 train accuracy: 0.159 test accuracy: 0.1639\n",
            "iteration number 99 has cost 2.216318406300292\n",
            "epoch 113 train accuracy: 0.161 test accuracy: 0.164\n",
            "iteration number 99 has cost 2.275247702553999\n",
            "epoch 114 train accuracy: 0.157 test accuracy: 0.1643\n",
            "iteration number 99 has cost 2.241544371906457\n",
            "epoch 115 train accuracy: 0.155 test accuracy: 0.1642\n",
            "iteration number 99 has cost 2.2512011623206862\n",
            "epoch 116 train accuracy: 0.183 test accuracy: 0.1642\n",
            "iteration number 99 has cost 2.240337000652328\n",
            "epoch 117 train accuracy: 0.137 test accuracy: 0.164\n",
            "iteration number 99 has cost 2.2233929507741497\n",
            "epoch 118 train accuracy: 0.16 test accuracy: 0.1644\n",
            "iteration number 99 has cost 2.2467854760447445\n",
            "epoch 119 train accuracy: 0.168 test accuracy: 0.1645\n",
            "iteration number 99 has cost 2.234614323589969\n",
            "epoch 120 train accuracy: 0.149 test accuracy: 0.1647\n",
            "iteration number 99 has cost 2.250732643770669\n",
            "epoch 121 train accuracy: 0.158 test accuracy: 0.1652\n",
            "iteration number 99 has cost 2.2461604411893092\n",
            "epoch 122 train accuracy: 0.184 test accuracy: 0.165\n",
            "iteration number 99 has cost 2.261544634625294\n",
            "epoch 123 train accuracy: 0.158 test accuracy: 0.1655\n",
            "iteration number 99 has cost 2.269537060849294\n",
            "epoch 124 train accuracy: 0.181 test accuracy: 0.166\n",
            "iteration number 99 has cost 2.2272826138415356\n",
            "epoch 125 train accuracy: 0.146 test accuracy: 0.1654\n",
            "iteration number 99 has cost 2.213680177382723\n",
            "epoch 126 train accuracy: 0.163 test accuracy: 0.1661\n",
            "iteration number 99 has cost 2.2200600894468447\n",
            "epoch 127 train accuracy: 0.155 test accuracy: 0.166\n",
            "iteration number 99 has cost 2.2532937436100364\n",
            "epoch 128 train accuracy: 0.142 test accuracy: 0.1662\n",
            "iteration number 99 has cost 2.253324951670139\n",
            "epoch 129 train accuracy: 0.163 test accuracy: 0.1664\n",
            "iteration number 99 has cost 2.213028195459587\n",
            "epoch 130 train accuracy: 0.162 test accuracy: 0.1664\n",
            "iteration number 99 has cost 2.2473554545172876\n",
            "epoch 131 train accuracy: 0.153 test accuracy: 0.1662\n",
            "iteration number 99 has cost 2.2469850209947673\n",
            "epoch 132 train accuracy: 0.159 test accuracy: 0.1662\n",
            "iteration number 99 has cost 2.258446089020618\n",
            "epoch 133 train accuracy: 0.162 test accuracy: 0.1668\n",
            "iteration number 99 has cost 2.255191908208767\n",
            "epoch 134 train accuracy: 0.181 test accuracy: 0.1665\n",
            "iteration number 99 has cost 2.236059305079966\n",
            "epoch 135 train accuracy: 0.153 test accuracy: 0.1669\n",
            "iteration number 99 has cost 2.2571254707509607\n",
            "epoch 136 train accuracy: 0.163 test accuracy: 0.167\n",
            "iteration number 99 has cost 2.249090929189815\n",
            "epoch 137 train accuracy: 0.175 test accuracy: 0.1672\n",
            "iteration number 99 has cost 2.284397826870879\n",
            "epoch 138 train accuracy: 0.165 test accuracy: 0.1674\n",
            "iteration number 99 has cost 2.2319737870350336\n",
            "epoch 139 train accuracy: 0.157 test accuracy: 0.1674\n",
            "iteration number 99 has cost 2.2453124313725437\n",
            "epoch 140 train accuracy: 0.182 test accuracy: 0.1678\n",
            "iteration number 99 has cost 2.243228015338959\n",
            "epoch 141 train accuracy: 0.178 test accuracy: 0.168\n",
            "iteration number 99 has cost 2.2339340935094136\n",
            "epoch 142 train accuracy: 0.139 test accuracy: 0.1684\n",
            "iteration number 99 has cost 2.2256800299012567\n",
            "epoch 143 train accuracy: 0.165 test accuracy: 0.1684\n",
            "iteration number 99 has cost 2.2439613752602554\n",
            "epoch 144 train accuracy: 0.156 test accuracy: 0.1684\n",
            "iteration number 99 has cost 2.2327578853530237\n",
            "epoch 145 train accuracy: 0.164 test accuracy: 0.1682\n",
            "iteration number 99 has cost 2.2634048208180255\n",
            "epoch 146 train accuracy: 0.177 test accuracy: 0.1684\n",
            "iteration number 99 has cost 2.2292980753846763\n",
            "epoch 147 train accuracy: 0.177 test accuracy: 0.1687\n",
            "iteration number 99 has cost 2.2528678346763753\n",
            "epoch 148 train accuracy: 0.167 test accuracy: 0.1688\n",
            "iteration number 99 has cost 2.2372682134315864\n",
            "epoch 149 train accuracy: 0.169 test accuracy: 0.1688\n",
            "iteration number 99 has cost 2.2395815367732586\n",
            "epoch 150 train accuracy: 0.175 test accuracy: 0.1687\n",
            "iteration number 99 has cost 2.248431797577874\n",
            "epoch 151 train accuracy: 0.152 test accuracy: 0.1688\n",
            "iteration number 99 has cost 2.25581397903587\n",
            "epoch 152 train accuracy: 0.156 test accuracy: 0.1691\n",
            "iteration number 99 has cost 2.2283761718311585\n",
            "epoch 153 train accuracy: 0.147 test accuracy: 0.1693\n",
            "iteration number 99 has cost 2.227315893129683\n",
            "epoch 154 train accuracy: 0.163 test accuracy: 0.1693\n",
            "iteration number 99 has cost 2.2286622521501522\n",
            "epoch 155 train accuracy: 0.162 test accuracy: 0.17\n",
            "iteration number 99 has cost 2.2253520217484795\n",
            "epoch 156 train accuracy: 0.169 test accuracy: 0.1695\n",
            "iteration number 99 has cost 2.236870371574236\n",
            "epoch 157 train accuracy: 0.151 test accuracy: 0.1698\n",
            "iteration number 99 has cost 2.2203270565670445\n",
            "epoch 158 train accuracy: 0.165 test accuracy: 0.1699\n",
            "iteration number 99 has cost 2.226015607426426\n",
            "epoch 159 train accuracy: 0.155 test accuracy: 0.1704\n",
            "iteration number 99 has cost 2.2157761983948197\n",
            "epoch 160 train accuracy: 0.162 test accuracy: 0.1703\n",
            "iteration number 99 has cost 2.2257111560983294\n",
            "epoch 161 train accuracy: 0.165 test accuracy: 0.1708\n",
            "iteration number 99 has cost 2.2232913215401213\n",
            "epoch 162 train accuracy: 0.159 test accuracy: 0.1708\n",
            "iteration number 99 has cost 2.2405422849830083\n",
            "epoch 163 train accuracy: 0.174 test accuracy: 0.1708\n",
            "iteration number 99 has cost 2.23923195864562\n",
            "epoch 164 train accuracy: 0.161 test accuracy: 0.1709\n",
            "iteration number 99 has cost 2.2083331442879324\n",
            "epoch 165 train accuracy: 0.155 test accuracy: 0.1713\n",
            "iteration number 99 has cost 2.239898160830707\n",
            "epoch 166 train accuracy: 0.169 test accuracy: 0.1712\n",
            "iteration number 99 has cost 2.2353586459435926\n",
            "epoch 167 train accuracy: 0.162 test accuracy: 0.1714\n",
            "iteration number 99 has cost 2.2446028885901685\n",
            "epoch 168 train accuracy: 0.154 test accuracy: 0.172\n",
            "iteration number 99 has cost 2.232189368158477\n",
            "epoch 169 train accuracy: 0.159 test accuracy: 0.1722\n",
            "iteration number 99 has cost 2.251070711525437\n",
            "epoch 170 train accuracy: 0.154 test accuracy: 0.1725\n",
            "iteration number 99 has cost 2.2014941234976293\n",
            "epoch 171 train accuracy: 0.16 test accuracy: 0.1724\n",
            "iteration number 99 has cost 2.2258535378844484\n",
            "epoch 172 train accuracy: 0.156 test accuracy: 0.1727\n",
            "iteration number 99 has cost 2.233215189178285\n",
            "epoch 173 train accuracy: 0.176 test accuracy: 0.1732\n",
            "iteration number 99 has cost 2.2408966819562055\n",
            "epoch 174 train accuracy: 0.197 test accuracy: 0.1726\n",
            "iteration number 99 has cost 2.241020545291667\n",
            "epoch 175 train accuracy: 0.154 test accuracy: 0.1732\n",
            "iteration number 99 has cost 2.2454332620630626\n",
            "epoch 176 train accuracy: 0.158 test accuracy: 0.1735\n",
            "iteration number 99 has cost 2.237397374419884\n",
            "epoch 177 train accuracy: 0.139 test accuracy: 0.1737\n",
            "iteration number 99 has cost 2.2229031640712305\n",
            "epoch 178 train accuracy: 0.175 test accuracy: 0.174\n",
            "iteration number 99 has cost 2.25340845804894\n",
            "epoch 179 train accuracy: 0.145 test accuracy: 0.1745\n",
            "iteration number 99 has cost 2.2294275364476546\n",
            "epoch 180 train accuracy: 0.184 test accuracy: 0.1745\n",
            "iteration number 99 has cost 2.2326706676287613\n",
            "epoch 181 train accuracy: 0.18 test accuracy: 0.1751\n",
            "iteration number 99 has cost 2.2185723615588815\n",
            "epoch 182 train accuracy: 0.177 test accuracy: 0.1751\n",
            "iteration number 99 has cost 2.2214531148573133\n",
            "epoch 183 train accuracy: 0.163 test accuracy: 0.1754\n",
            "iteration number 99 has cost 2.249260423282904\n",
            "epoch 184 train accuracy: 0.174 test accuracy: 0.1755\n",
            "iteration number 99 has cost 2.2049900461434864\n",
            "epoch 185 train accuracy: 0.172 test accuracy: 0.1758\n",
            "iteration number 99 has cost 2.2519939847517336\n",
            "epoch 186 train accuracy: 0.195 test accuracy: 0.1759\n",
            "iteration number 99 has cost 2.223539339340188\n",
            "epoch 187 train accuracy: 0.14 test accuracy: 0.1763\n",
            "iteration number 99 has cost 2.238681152526232\n",
            "epoch 188 train accuracy: 0.181 test accuracy: 0.1766\n",
            "iteration number 99 has cost 2.2328011896271316\n",
            "epoch 189 train accuracy: 0.187 test accuracy: 0.1766\n",
            "iteration number 99 has cost 2.2464820939749193\n",
            "epoch 190 train accuracy: 0.175 test accuracy: 0.1762\n",
            "iteration number 99 has cost 2.2055109739983307\n",
            "epoch 191 train accuracy: 0.156 test accuracy: 0.1766\n",
            "iteration number 99 has cost 2.2334381755340247\n",
            "epoch 192 train accuracy: 0.181 test accuracy: 0.1773\n",
            "iteration number 99 has cost 2.2104970612307904\n",
            "epoch 193 train accuracy: 0.17 test accuracy: 0.1772\n",
            "iteration number 99 has cost 2.2468379324268297\n",
            "epoch 194 train accuracy: 0.18 test accuracy: 0.1772\n",
            "iteration number 99 has cost 2.2327245767603108\n",
            "epoch 195 train accuracy: 0.165 test accuracy: 0.1769\n",
            "iteration number 99 has cost 2.2111029900364882\n",
            "epoch 196 train accuracy: 0.173 test accuracy: 0.1772\n",
            "iteration number 99 has cost 2.2074718947389287\n",
            "epoch 197 train accuracy: 0.183 test accuracy: 0.177\n",
            "iteration number 99 has cost 2.2350268505610433\n",
            "epoch 198 train accuracy: 0.172 test accuracy: 0.1777\n",
            "iteration number 99 has cost 2.2238793968254464\n",
            "epoch 199 train accuracy: 0.152 test accuracy: 0.1776\n",
            "iteration number 99 has cost 2.212338790552415\n",
            "epoch 200 train accuracy: 0.157 test accuracy: 0.1774\n",
            "iteration number 99 has cost 2.253015541505022\n",
            "epoch 201 train accuracy: 0.178 test accuracy: 0.178\n",
            "iteration number 99 has cost 2.2270439073876083\n",
            "epoch 202 train accuracy: 0.154 test accuracy: 0.1781\n",
            "iteration number 99 has cost 2.233384196187777\n",
            "epoch 203 train accuracy: 0.174 test accuracy: 0.178\n",
            "iteration number 99 has cost 2.22706930428957\n",
            "epoch 204 train accuracy: 0.16 test accuracy: 0.1782\n",
            "iteration number 99 has cost 2.211609328886601\n",
            "epoch 205 train accuracy: 0.155 test accuracy: 0.1781\n",
            "iteration number 99 has cost 2.2464232399464206\n",
            "epoch 206 train accuracy: 0.163 test accuracy: 0.1781\n",
            "iteration number 99 has cost 2.203410284626977\n",
            "epoch 207 train accuracy: 0.158 test accuracy: 0.178\n",
            "iteration number 99 has cost 2.2200203066097064\n",
            "epoch 208 train accuracy: 0.202 test accuracy: 0.1781\n",
            "iteration number 99 has cost 2.229368028461126\n",
            "epoch 209 train accuracy: 0.179 test accuracy: 0.1783\n",
            "iteration number 99 has cost 2.2314342075369495\n",
            "epoch 210 train accuracy: 0.176 test accuracy: 0.1785\n",
            "iteration number 99 has cost 2.246748864489668\n",
            "epoch 211 train accuracy: 0.168 test accuracy: 0.1785\n",
            "iteration number 99 has cost 2.240209024336271\n",
            "epoch 212 train accuracy: 0.163 test accuracy: 0.1786\n",
            "iteration number 99 has cost 2.22297806758666\n",
            "epoch 213 train accuracy: 0.175 test accuracy: 0.1788\n",
            "iteration number 99 has cost 2.217768294675581\n",
            "epoch 214 train accuracy: 0.19 test accuracy: 0.1791\n",
            "iteration number 99 has cost 2.221163040932311\n",
            "epoch 215 train accuracy: 0.194 test accuracy: 0.1788\n",
            "iteration number 99 has cost 2.218023514205213\n",
            "epoch 216 train accuracy: 0.177 test accuracy: 0.179\n",
            "iteration number 99 has cost 2.2371082727426947\n",
            "epoch 217 train accuracy: 0.166 test accuracy: 0.1793\n",
            "iteration number 99 has cost 2.2129536433432806\n",
            "epoch 218 train accuracy: 0.146 test accuracy: 0.1799\n",
            "iteration number 99 has cost 2.237312064209089\n",
            "epoch 219 train accuracy: 0.175 test accuracy: 0.1794\n",
            "iteration number 99 has cost 2.206155833765957\n",
            "epoch 220 train accuracy: 0.166 test accuracy: 0.1798\n",
            "iteration number 99 has cost 2.2405649027122747\n",
            "epoch 221 train accuracy: 0.171 test accuracy: 0.1802\n",
            "iteration number 99 has cost 2.2306254012038416\n",
            "epoch 222 train accuracy: 0.164 test accuracy: 0.1801\n",
            "iteration number 99 has cost 2.235835001842392\n",
            "epoch 223 train accuracy: 0.179 test accuracy: 0.1801\n",
            "iteration number 99 has cost 2.245960217928522\n",
            "epoch 224 train accuracy: 0.175 test accuracy: 0.1802\n",
            "iteration number 99 has cost 2.2072842456030095\n",
            "epoch 225 train accuracy: 0.159 test accuracy: 0.1803\n",
            "iteration number 99 has cost 2.231390633310558\n",
            "epoch 226 train accuracy: 0.173 test accuracy: 0.1807\n",
            "iteration number 99 has cost 2.23108692831752\n",
            "epoch 227 train accuracy: 0.154 test accuracy: 0.1806\n",
            "iteration number 99 has cost 2.1959771009272826\n",
            "epoch 228 train accuracy: 0.175 test accuracy: 0.1807\n",
            "iteration number 99 has cost 2.2503706988441383\n",
            "epoch 229 train accuracy: 0.176 test accuracy: 0.1807\n",
            "iteration number 99 has cost 2.225735389091664\n",
            "epoch 230 train accuracy: 0.178 test accuracy: 0.1808\n",
            "iteration number 99 has cost 2.2175480630127637\n",
            "epoch 231 train accuracy: 0.163 test accuracy: 0.1806\n",
            "iteration number 99 has cost 2.2205347456322686\n",
            "epoch 232 train accuracy: 0.173 test accuracy: 0.1806\n",
            "iteration number 99 has cost 2.222330485905115\n",
            "epoch 233 train accuracy: 0.175 test accuracy: 0.1811\n",
            "iteration number 99 has cost 2.2332839823391124\n",
            "epoch 234 train accuracy: 0.192 test accuracy: 0.181\n",
            "iteration number 99 has cost 2.2336971911180727\n",
            "epoch 235 train accuracy: 0.175 test accuracy: 0.1815\n",
            "iteration number 99 has cost 2.2570718429719037\n",
            "epoch 236 train accuracy: 0.18 test accuracy: 0.1812\n",
            "iteration number 99 has cost 2.215608988792124\n",
            "epoch 237 train accuracy: 0.175 test accuracy: 0.1812\n",
            "iteration number 99 has cost 2.2330604676208594\n",
            "epoch 238 train accuracy: 0.18 test accuracy: 0.1813\n",
            "iteration number 99 has cost 2.239851514344672\n",
            "epoch 239 train accuracy: 0.146 test accuracy: 0.1815\n",
            "iteration number 99 has cost 2.230063351781093\n",
            "epoch 240 train accuracy: 0.189 test accuracy: 0.1812\n",
            "iteration number 99 has cost 2.231600162395165\n",
            "epoch 241 train accuracy: 0.183 test accuracy: 0.1821\n",
            "iteration number 99 has cost 2.2144621888373166\n",
            "epoch 242 train accuracy: 0.163 test accuracy: 0.182\n",
            "iteration number 99 has cost 2.223659993613239\n",
            "epoch 243 train accuracy: 0.165 test accuracy: 0.1819\n",
            "iteration number 99 has cost 2.26310808760709\n",
            "epoch 244 train accuracy: 0.197 test accuracy: 0.1817\n",
            "iteration number 99 has cost 2.218001553234537\n",
            "epoch 245 train accuracy: 0.169 test accuracy: 0.1822\n",
            "iteration number 99 has cost 2.2312266057444825\n",
            "epoch 246 train accuracy: 0.196 test accuracy: 0.1825\n",
            "iteration number 99 has cost 2.204844722836676\n",
            "epoch 247 train accuracy: 0.161 test accuracy: 0.1826\n",
            "iteration number 99 has cost 2.2221983143164925\n",
            "epoch 248 train accuracy: 0.164 test accuracy: 0.1831\n",
            "iteration number 99 has cost 2.2157598765662216\n",
            "epoch 249 train accuracy: 0.167 test accuracy: 0.1827\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN2BooIJmU27",
        "colab_type": "code",
        "outputId": "f98ecd44-4335-4305-8fd3-5b116fa1ef42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "  X = np.zeros((3072))\n",
        "  num_features, = X.shape\n",
        "  weights = np.zeros((3, 4000, 3999))\n",
        "  num_layers, num_units_in, num_units_out = weights.shape\n",
        "  a = np.ones((num_layers-1, num_units_in))\n",
        "  z = np.zeros((num_layers-1, num_units_out))\n",
        "  y = np.zeros(10)\n",
        "  deltas = np.zeros((num_layers+1, num_units_in))\n",
        "  mu = np.zeros(num_layers)\n",
        "  var = np.zeros(num_layers)\n",
        "  # layer 0\n",
        "  out = np.dot(X, weights[0,0:num_features,:])\n",
        "  print(out)\n",
        "  ypre = np.dot(a[num_layers-2,:], weights[num_layers-1,:,:10])\n",
        "  print(ypre.shape)\n",
        "  y = np.exp(ypre) / np.sum(np.exp(ypre), axis=0) \n",
        "  print(y.shape)\n",
        "  y = softmax(X)\n",
        "  print(y.shape)\n",
        "  print('relu', relu_deriv(z[-1, :]).shape)\n",
        "  i=1\n",
        "  non_lin_deriv = relu_deriv\n",
        "  s = np.dot(deltas[-i+1, :], (weights[-i, :,:]))*non_lin_deriv(z[-i, :])\n",
        "  q = np.dot(s, np.transpose(a[-i, 1:]))\n",
        "  print(s.shape)\n",
        "  print(q.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "(10,)\n",
            "(10,)\n",
            "(3072,)\n",
            "relu (3999,)\n",
            "(3999,)\n",
            "()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWJyLBRrPbco",
        "colab_type": "code",
        "outputId": "001d7852-bd35-477e-e741-618f60b6e39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = np.array([1,1])\n",
        "w = np.array([[2, 3, 4],\n",
        "             [5, 6, 7]])\n",
        "yhat = np.array([[0, 1, 2], \n",
        "                 [3, 4, 1]])\n",
        "y = np.array([[0, 1, 2], \n",
        "                 [3, 4, 5]])\n",
        "\n",
        "print(np.max(yhat, axis=1))\n",
        "print(np.max(y, axis = 1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4]\n",
            "[2 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX3zHTBeTLe4",
        "colab_type": "code",
        "outputId": "e5062fd2-b344-493d-f377-2b5307512902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "source": [
        "# print(1)\n",
        "# mini_batch_grad_desc(500, 50, 100, \n",
        "#                          sigmoid, sigmoid_deriv, \n",
        "#                          100, lr=0.001, batch_norm = False)\n",
        "print(2)\n",
        "mini_batch_grad_desc(10, 50, 4000, \n",
        "                         relu, relu_deriv, \n",
        "                         100, lr=0.001, batch_norm = False)\n",
        "print(3)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         leaky_relu, leaky_relu_deriv, \n",
        "                         100, lr=0.001, batch_norm = False)\n",
        "print(4)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         relu, relu_deriv, \n",
        "                         100, lr=0.001, batch_norm = False)\n",
        "print(11)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         sigmoid, sigmoid_deriv, \n",
        "                         100, lr=0.01, batch_norm = False)\n",
        "print(12)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         relu, relu_deriv, \n",
        "                         100, lr=0.01, batch_norm = False)\n",
        "print(13)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         leaky_relu, leaky_relu_deriv, \n",
        "                         100, lr=0.01, batch_norm = False)\n",
        "print(14)\n",
        "mini_batch_grad_desc(500, 50, 200, \n",
        "                         relu, relu_deriv, \n",
        "                         100, lr=0.01, batch_norm = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "iteration number 1 has cost 18.282993470788476\n",
            "weights [-0.03887523  0.09063869  0.04424523 -0.14081023]\n",
            "iteration number 99 has cost 10.802095363596887\n",
            "weights [-0.04639922  0.08291351  0.03598628 -0.14761918]\n",
            "epoch 0 train accuracy: 0.169 test accuracy: 0.1718\n",
            "iteration number 1 has cost 17.97967195355303\n",
            "weights [-0.04640677  0.08292818  0.03596133 -0.14762935]\n",
            "iteration number 99 has cost 14.537182014251737\n",
            "weights [-0.04625677  0.08263937  0.03602084 -0.14702994]\n",
            "epoch 1 train accuracy: 0.159 test accuracy: 0.1568\n",
            "iteration number 1 has cost 13.999704699027864\n",
            "weights [-0.04627998  0.08262892  0.03601482 -0.14702095]\n",
            "iteration number 99 has cost 22.204537656124238\n",
            "weights [-0.04598296  0.08159936  0.0358726  -0.14627387]\n",
            "epoch 2 train accuracy: 0.204 test accuracy: 0.2027\n",
            "iteration number 1 has cost 22.792349363270652\n",
            "weights [-0.04594935  0.08153604  0.03587556 -0.14625695]\n",
            "iteration number 99 has cost 21.623265677209005\n",
            "weights [-0.04576767  0.08062146  0.03570961 -0.14529067]\n",
            "epoch 3 train accuracy: 0.2 test accuracy: 0.2043\n",
            "iteration number 1 has cost 21.091935660840733\n",
            "weights [-0.04575544  0.08061573  0.03571849 -0.14529015]\n",
            "iteration number 99 has cost 19.36699119925641\n",
            "weights [-0.04577385  0.08011251  0.03509958 -0.14447086]\n",
            "epoch 4 train accuracy: 0.221 test accuracy: 0.1851\n",
            "iteration number 1 has cost 16.28361916968189\n",
            "weights [-0.04574735  0.08008334  0.03508403 -0.14445743]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-b5abad860199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m mini_batch_grad_desc(10, 50, 4000, \n\u001b[1;32m      3\u001b[0m                          \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_deriv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                          100, lr=0.001, batch_norm = False)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m mini_batch_grad_desc(500, 50, 200, \n",
            "\u001b[0;32m<ipython-input-151-e2abb3443f3c>\u001b[0m in \u001b[0;36mmini_batch_grad_desc\u001b[0;34m(batch_size, num_epochs, num_units, non_lin, non_lin_deriv, num_iter, lr, batch_norm)\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mcum_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m       \u001b[0mcum_dgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mcum_dbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-7742ff8c36b0>\u001b[0m in \u001b[0;36mrand_init\u001b[0;34m(num_layers, num_units, zero, bias)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mweights1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}